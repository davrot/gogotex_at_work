import { writeFileSync, readFileSync, accessSync, constants } from 'node:fs'
import path from 'node:path'
import { execSync } from 'node:child_process'

const base = path.resolve(import.meta.dirname, '../../frontend/js/features/source-editor')
const gramDir = path.join(base, 'lezer-latex')
const grammars = [
  {
    grammarPath: path.join(gramDir, 'latex.grammar'),
    parserOutputPath: path.join(gramDir, 'latex.mjs'),
    termsOutputPath: path.join(gramDir, 'latex.terms.mjs'),
  },
  {
    grammarPath: path.join(base, 'lezer-bibtex', 'bibtex.grammar'),
    parserOutputPath: path.join(base, 'lezer-bibtex', 'bibtex.mjs'),
    termsOutputPath: path.join(base, 'lezer-bibtex', 'bibtex.terms.mjs'),
  },
]

function exists(p) {
  try {
    accessSync(p, constants.R_OK)
    return true
  } catch (e) {
    return false
  }
}

function writeStubParser(p) {
  const content = `// Minimal generated stub - replace by running 'npm run lezer-latex:generate'
export const parser = {
  configure() { return this },
  parse() { return { cursor() { return { next() { return false }, done: true } } } },
}
export default parser
`
  writeFileSync(p, content)
}

function writeStubTerms(p, grammarText) {
  // Try to extract rule/token names heuristically from grammar
  const names = new Set()

  // from @external {...} blocks
  for (const m of grammarText.matchAll(/@external[^{]*\{([^}]+)\}/g)) {
    const list = m[1].split(',').map(s => s.trim()).filter(Boolean)
    for (const n of list) names.add(n)
  }

  // from rules: lines starting with identifier
  for (const line of grammarText.split(/\r?\n/)) {
    const m = line.match(/^\s*([A-Za-z][A-Za-z0-9_]*)\b/) // rule or token
    if (m) {
      const nm = m[1]
      // ignore keywords like @tokens, @external, @top, etc
      if (!nm.startsWith('@') && nm.length > 1) names.add(nm)
    }
  }

  // Ensure commonly used section names exist
  ;['Book','Part','Chapter','Section','SubSection','SubSubSection','Paragraph','SubParagraph','Frame','NewLine','Whitespace','BlankLine'].forEach(n=>names.add(n))

  let i = 1
  const lines = [
    `// Minimal terms stub generated by ensure-generated.mjs. Run 'npm run lezer-latex:generate' to regenerate real files.`,
  ]
  for (const n of Array.from(names).sort()) {
    lines.push(`export const ${n} = ${i++}`)
  }
  lines.push('\nexport default {')
  const props = Array.from(names).sort().map(n => `  ${n},`).join('\n')
  lines.push(props)
  lines.push('}')

  writeFileSync(p, lines.join('\n'))
}

let usedFallback = false
for (const g of grammars) {
  if (exists(g.parserOutputPath) && exists(g.termsOutputPath)) {
    console.info('OK:', g.parserOutputPath)
    continue
  }
  // try to run the real generator
  try {
    console.info('Attempting to run lezer generator...')
    execSync('node scripts/lezer-latex/generate.mjs', { stdio: 'inherit' })
    // verify again
    if (exists(g.parserOutputPath) && exists(g.termsOutputPath)) {
      console.info('Generator produced outputs')
      continue
    }
    console.warn('Generator did not produce expected outputs; falling back to stubs')
  } catch (err) {
    console.warn('Generator failed or is unavailable; falling back to stubs')
  }

  // fallback: create simple stubs
  usedFallback = true
  const grammarText = exists(g.grammarPath) ? readFileSync(g.grammarPath, 'utf8') : ''

  if (!exists(g.parserOutputPath)) {
    console.info('Writing stub parser:', g.parserOutputPath)
    writeStubParser(g.parserOutputPath)
  }
  if (!exists(g.termsOutputPath)) {
    console.info('Writing stub terms:', g.termsOutputPath)
    writeStubTerms(g.termsOutputPath, grammarText)
  }
}

if (usedFallback) {
  console.info('\nWrote minimal lezer stubs. Tests can now run, but behavior may differ from the real parser. To regenerate the real artifacts run:')
  console.info('  cd services/web && npm run lezer-latex:generate')
}

process.exit(0)
